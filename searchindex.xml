<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>Kubernetes简介</title><url>https://www.etaon.link/2022/01/02/kubernetes-indor.html</url><categories><category>Kubernetes</category></categories><tags><tag>测试</tag><tag>学习</tag></tags><content type="html"> Kubernetes概述 微服务的分离式设计与容器的原子性结合，使你可以通过部署更多容器实例来横向扩展应用以及响应增加的需求，并在需求减少时横向缩减。
容器业务流程协调程序是自动部署和管理容器化应用的系统。 例如，业务流程协调程序可以动态响应环境中的变化，以增加或减少部署的托管应用实例。 或者，如果发布了新版本的服务，则它可以确保所有已部署的容器实例得到更新。
Kubernetes 是一种可移植且可扩展的开放源代码平台，用于管理和编排容器化工作负载。 Kubernetes 不考虑复杂的容器管理任务，并提供声明性配置，以便在不同的计算环境中协调容器。
任务包括：
容器的自行修复。 例如，重启失败的容器或替换容器。 根据需要动态地纵向扩展或纵向缩减部署的容器计数。 容器的自动滚动更新和回滚。 管理存储。 管理网络流量。 存储并管理敏感信息，如用户名和密码。 Kubernetes 体系结构
Kubernetes 群集至少包含一个主平面以及一个或多个节点。 控制平面和节点实例都可以是物理设备、虚拟机或云中的实例。
Kubernetes 群集中的 Kubernetes 控制平面运行一个服务集合，该集合管理 Kubernetes 中的业务流程功能。
API 服务器 (API Server) 可以将 API 服务器视为 Kubernetes 群集中控制平面的前端。 此 API 公开一个 RESTful API，通过该 API 可以发布命令或基于 YAML 的配置文件。 YAML 是编程语言的可读数据序列化标准。 使用 YAML 文件可定义 Kubernetes 群集中所有对象的预期状态。 后备存储(ETCD) 后备存储是一个持久性存储，Kubernetes 群集使用它来保存 Kubernetes 群集的完整配置。 Kubernetes 使用名为 etcd 的可靠的高可用性分布式键值存储。 此键值存储存储当前状态以及群集中所有对象的所需状态。 计划程序(Scheduler) 计划程序是负责在所有节点间分配工作负载的组件。 计划程序监视群集中是否有新创建的容器，并将它们分配给节点。 控制器管理器(Controller Manager) 控制器管理器通过 API 服务器启动和监视为群集配置的控制器。 Kubernetes 利用控制器跟踪群集中对象的状态。 在监视和响应群集中的事件的过程中，每个控制器都在非终止循环中运行。 例如，有一些控制器用于监视节点、容器和终结点。 控制器与 API 服务器通信，以确定对象的状态。 如果对象的当前状态与所需状态不一致，则控制器将采取措施来确保所需状态。 云控制器管理器 群集在云环境中运行时，云控制器管理器会与群集中的基础云技术集成。 例如，这些服务可以是负载均衡器、队列和存储。 控制工作负载的工作由节点node处理，以下服务在 Kubernetes 节点上运行：
kubelet kubelet 是一个代理，在群集的每个节点上运行，并监视 API 服务器的工作请求。kubelet 监视节点， 它确保请求的工作单元正在运行并一切正常。 kubelet 仅管理由 Kubernetes 创建的容器。 在当前节点无法运行工作时，它不负责将工作重新安排到其他节点上运行。 kube-proxy kube-proxy 组件负责本地群集网络，并在每个节点上运行。 它确保每个节点都具有唯一的 IP 地址。 它还实行使用 iptables 和 IPVS 处理流量的路由和负载平衡的规则。 此代理本身不提供 DNS 服务。 容器运行时 容器运行时是在 Kubernetes 群集上运行容器的基础软件。 运行时负责提取、启动和停止容器映像。 Kubernetes 支持多种容器运行时，包括但不限于 Docker、rkt、CRI-O、containerd 和 frakti。 对多种容器运行时类型的支持都是基于容器运行时接口 (CRI) 实现的。 Kubernetes 提供了一个名为 kubectl 的命令行工具，可用于管理群集。 使用 kubectl 将命令发送到群集的控制平面，或通过 API 服务器获取所有 Kubernetes 对象的信息。
Kubernetes pod Pod 表示在 Kubernetes 中运行的应用的单个实例。 在 Kubernetes 上运行的工作负载是容器化应用。 与 Docker 环境不同，不能直接在 Kubernetes 上运行容器。 请将容器打包到称为 Pod 的 Kubernetes 对象中。 Pod 是 Kubernetes 中可创建的最小对象。单个 Pod 可容纳一组容器（可以是一个或多个容器）。
Pod 生命周期
各个阶段：
挂起的 计划 Pod 运行后，容器运行时将下载容器映像并启动 Pod 的所有容器。 Column 运行 Pod 中的所有资源准备就绪后，Pod 会转变为运行状态。 成功 Pod 完成其预期任务并成功运行后，Pod 会转变为成功状态。 失败 Pod 可能因多种原因而失败。 Pod 中的容器可能已失败，导致所有其他容器终止。 或者可能在准备 Pod 容器期间未找到映像。 在这些情况下，Pod 可能转变为失败状态。 Pod 可从挂起状态或运行状态转变为失败状态。 特定的失败还可以将一个 Pod 变回挂起状态。 未知 如果无法确定 Pod 的状态，则该 Pod 处于未知状态。 Pod 会一直保存在群集上，直到控制器、控制平面或用户显式地删除它们。 删除 Pod 并将其替换为新的 Pod 后，根据 Pod 清单，这个新的 Pod 就是该 Pod 的一个全新实例。
群集不会保存 Pod 的状态或动态分配到的配置。 例如，它不会保存 Pod 的 ID 或 IP 地址。 这一点会影响部署 Pod 的方式以及应用的设计方式。 例如，你不能依赖于 Pod 预先分配到的 IP 地址。
检查 Pod 时，群集会使用三种状态来跟踪 Pod 中的容器：
等待 这是容器的默认状态，以及容器在未运行或未终止时的状态。 运行 容器正在按预期运行，未出现任何问题。 终止 容器不再运行。 原因是所有任务都已完成或容器因某种原因而失败。 原因和退出代码可用于调试这两种情况。 Kubernetes 是一种可移植且可扩展的开放源代码平台，可用于自动进行部署、缩放和管理容器化的工作负载。 Kubernetes 不考虑复杂的容器管理，并为我们提供声明性配置，以便在不同的计算环境中协调容器。 与平台即服务 (PaaS) 和基础结构即服务 (IaaS) 产品/服务相比，此业务流程平台提供了相同的易用性和灵活性。
借助 Kubernetes，你可以将数据中心视为一台大型计算机。 无需担心部署容器的方式和位置，只需按需部署和缩放应用程序。
但是，这种观点可能稍微有些误导性，因为需谨记以下几个方面：
Kubernetes 不是完整的 PaaS 产品/服务。 它在容器级别运行，并且仅提供一组常用 PaaS 功能。 Kubernetes 不是一个整体。 它不是安装的单个应用程序。 部署、缩放、负载均衡、日志记录和监视等方面都是可选的。 由你负责寻找最适合自己需求的解决方案来解决这些方面的问题。 Kubernetes 不限制可运行的应用程序类型。 如果应用程序可以在容器中运行，也就可以在 Kubernetes 上运行。 开发人员需要了解微服务体系结构等概念，以便充分利用容器解决方案。 Kubernetes 不提供中间件、数据处理框架、数据库、缓存，也不提供群集存储系统。 所有这些项都作为容器或其他服务产品的一部分运行。 Kubernetes 部署配置为群集。 一个群集包含至少一台主计算机以及一台或多台辅助角色计算机。 对于生产部署，首选主配置是具有 3 到 5 个复制主机的多主高可用性部署。 这些计算机可以是物理硬件，也可以是 VM。 这些辅助角色计算机被称为节点或代理节点。</content></entry><entry><title>Redis Cluster</title><url>https://www.etaon.link/2022/01/02/redis-cluster.html</url><categories><category>redis</category></categories><tags><tag>redis</tag><tag>学习</tag></tags><content type="html"> 概述 单机/单主的性能瓶颈
IOPS 约100k/s读，20-80k/s写 单机内存限制 Cluster
访问，负载均衡 内存，扩展 灾难恢复 集群存储空间设计 设计原理 Redis cluster 采用虚拟哈希槽分区，所有的键根据哈希函数映射到 0 ~ 16383 整数槽内，每个key通过crc16校验后对16384取模来决定放置哪个槽(slot)，每一个节点负责维护一部分槽以及槽所映射的键值数据。
计算公式：slot = crc16(key) % 16383。
这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。 当需要增加节点时，把其他节点的某些哈希槽挪到新节点； 当需要移除节点时，把移除节点上的哈希槽挪到其他节点。
参考文档：http://www.zzvips.com/article/205769.html
为什么设计成16384（2^14）个槽位，作者给出了解释 https://github.com/antirez/redis/issues/2576
1.如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。
如上所述，在消息头中，最占空间的是myslots[cluster_slots/8]。 当槽位为65536时，这块的大小是:65536÷8÷1024=8kb因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。
2.redis的集群主节点数量基本不可能超过1000个。 如上所述，集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。 那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。
3.槽位越小，节点少的情况下，压缩率高 redis主节点的配置信息中，它所负责的哈希槽是通过一张bitmap的形式来保存的，在传输过程中，会对bitmap进行压缩，但是如果bitmap的填充率slots / n很高的话(n表示节点数)，bitmap的压缩率就很低。 如果节点数很少，而哈希槽数量很多的话，bitmap的压缩率就很低。 而16384÷8÷1024=2kb
综上所述，作者决定取16384个槽。
集群内部通信 每个节点相互通信，保存各自库中槽的编号数据 一次命中，直接返回 一次未命中，告知具体位置 如上图
![https://img-blog.csdnimg.cn/7d51ff4081534ee4886e753fc1408412.png](https://img-blog.csdnimg.cn/7d51ff4081534ee4886e753fc1408412.png) 在192.168.21.12:6380查询list01时，一次命中 查询zset01时，未命中，告知并跳转到192.168.21.13:6381，返回结果 Lab实验 环境设置 初始化
Untitled
配置命令 port 6380 daemonize no bind 0.0.0.0 dir &ldquo;/root/redis-6.0.6/data&rdquo; protected-mode no cluster-enabled yescluster-config-file node-6380.confcluster-node-timeout 10000
启动集群 6 Server按conf启动 命令启动 [root@worker-01 conf]# ../src/redis-cli &ndash;cluster create 192.168.21.11:6379 192.168.21.12:6380 192.168.21.13:6381 192.168.21.12:6383 192.168.21.13:6384 192.168.21.11:6382 &ndash;cluster-replicas 1
最后的参数 1 表示集群中master的slave数为1 按照列出的server前后顺序分配master和slave Performing hash slots allocation on 6 nodes&hellip; Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 192.168.21.12:6383 to 192.168.21.11:6379 Adding replica 192.168.21.13:6384 to 192.168.21.12:6380 Adding replica 192.168.21.11:6382 to 192.168.21.13:6381 M: b9b0603f8995f275576f4e9ad2b98708b13ba40e 192.168.21.11:6379 slots:[0-5460] (5461 slots) master M: e0379031bbe11a1ad523910d9a3d72618331e930 192.168.21.12:6380 slots:[5461-10922] (5462 slots) master M: 01215826194d3b2fc182b32465d443a85ad11143 192.168.21.13:6381 slots:[10923-16383] (5461 slots) master S: ccd38a4b40a50048b969bba946bbf1cc16f21a96 192.168.21.12:6383 replicates b9b0603f8995f275576f4e9ad2b98708b13ba40e S: d5c0f25036222406817bc9bac9bab4f30cdfa86b 192.168.21.13:6384 replicates e0379031bbe11a1ad523910d9a3d72618331e930 S: f21a44755356bbaa416f4ee61092f4bb7052ef69 192.168.21.11:6382 replicates 01215826194d3b2fc182b32465d443a85ad11143
Can I set the above configuration? (type &lsquo;yes&rsquo; to accept): yes
Nodes configuration updated Assign a different config epoch to each node Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join .
Performing Cluster Check (using node 192.168.21.11:6379) M: b9b0603f8995f275576f4e9ad2b98708b13ba40e 192.168.21.11:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s) M: 01215826194d3b2fc182b32465d443a85ad11143 192.168.21.13:6381 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: f21a44755356bbaa416f4ee61092f4bb7052ef69 192.168.21.11:6382 slots: (0 slots) slave replicates 01215826194d3b2fc182b32465d443a85ad11143 S: ccd38a4b40a50048b969bba946bbf1cc16f21a96 192.168.21.12:6383 slots: (0 slots) slave replicates b9b0603f8995f275576f4e9ad2b98708b13ba40e M: e0379031bbe11a1ad523910d9a3d72618331e930 192.168.21.12:6380 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: d5c0f25036222406817bc9bac9bab4f30cdfa86b 192.168.21.13:6384 slots: (0 slots) slave replicates e0379031bbe11a1ad523910d9a3d72618331e930 All nodes agree about slots configuration.
Check for open slots&hellip; Check slots coverage&hellip; All 16384 slots covered.
系统自动将Cluster配置好。 同时，定义好的系统文件被更改，记录下系统配置情况：
查看一组master/slave的log： Master-6379： Slave-6383：
模拟灾难恢复 停止Master-6380
查看Slave，timeout=10s
10s内找Master同步，过了timeout时间，系统failover机制作用将6384变成Master
![https://img-blog.csdnimg.cn/848ca172f42648708730c48bf79625c0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBARXRhb24=,size_20,color_FFFFFF,t_70,g_se,x_16](https://img-blog.csdnimg.cn/848ca172f42648708730c48bf79625c0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBARXRhb24=,size_20,color_FFFFFF,t_70,g_se,x_16) 其他server的配置发生改变 Master-6380已经标记为fail，6384变为Master
![https://img-blog.csdnimg.cn/64861c80f2db4190b908b12e2b956d4d.png](https://img-blog.csdnimg.cn/64861c80f2db4190b908b12e2b956d4d.png) 重新启动6380 发现其连接到Master-6384，成为Slave
![https://img-blog.csdnimg.cn/b79250bd9b474527b4b44c2beb04002c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBARXRhb24=,size_20,color_FFFFFF,t_70,g_se,x_16](https://img-blog.csdnimg.cn/b79250bd9b474527b4b44c2beb04002c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBARXRhb24=,size_20,color_FFFFFF,t_70,g_se,x_16) 在配置文件上也反映了这一现象
6384上的log记录了这一过程
其他server同步记录下
在redis-cli模式下也可以通过命令查看最终节点情况
最终结果</content></entry><entry><title>关于我</title><url>https://www.etaon.link/about.html</url><categories/><tags/><content type="html"> Hugo使用了多种开源项目，包括:
https://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo是博客、企业网站、创意作品集、在线杂志、单页应用程序甚至是数千页的网站的理想选择。
Hugo适合那些想要手工编写自己的网站代码，而不用担心设置复杂的运行时、依赖关系和数据库的人。
使用Hugo建立的网站非常快速、安全，可以部署在任何地方，包括AWS、GitHub Pages、Heroku、Netlify和任何其他托管提供商。
更多信息请访问GitHub.</content></entry></search>